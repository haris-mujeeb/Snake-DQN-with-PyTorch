
# ðŸ Snake AI (Reinforcement Learning with PyTorch)

This project implements a reinforcement learning agent that learns to play the classic Snake game using **Deep Q-Learning (DQN)**. The environment is rendered using **Pygame**, and the agent is trained using **PyTorch**.

https://github.com/user-attachments/assets/0e02d945-14a2-4bdc-b5ac-d6971d97fddd

---

## ðŸš€ Overview

The agent learns to play Snake by trial and error using **Deep Q-learning**. It gets:

* ðŸŸ¢ **+10** for eating food
* ðŸ”´ **âˆ’10** for dying
* âšª **âˆ’0.1** for every move (to encourage efficient paths)
* â±ï¸ **âˆ’5.0** if it takes too long to reach food (timeout penalty)

The game logic is handled in a headless environment (BaseSnakeGame) without rendering, making training efficient. A Pygame version is used only for visualization during AI or human play.

The state fed to the neural network includes:
- ðŸš§ Danger detection (straight, left, right)
- âž¡ï¸ Current movement direction (one-hot encoded)
- ðŸŽ Relative position of food (dx, dy)
- ðŸ“ Manhattan distance to food
- ðŸ Snake length normalized by grid size

This concise state helps the agent learn to differentiate between walls, itself, and food, enabling smarter and more strategic movement.

---

## ðŸ§  Algorithm

We use the **Q-Learning** update rule:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

In Deep Q-Network (DQN), this is approximated as:

```python
target = reward + (0 if done else gamma * torch.max(target_model(next_state)))
```

---

## ðŸ“ Project Structure

```
â”œâ”€â”€ snake_game_core.py         # Core game logic without rendering (used for training)
â”œâ”€â”€ train_snake_ai.py          # Trains the DQN agent with optional pretrained model loading
â”œâ”€â”€ play_snake_ai.py           # Lets a trained AI model play the game (with Pygame rendering)
â”œâ”€â”€ play_snake_human.py        # Allows human to play Snake via keyboard
â”œâ”€â”€ models/                    # Saved PyTorch model files (.pth)
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ demo.mp4               # Gameplay demonstration
â”œâ”€â”€ plots/                     # Training performance plots (optional)
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ requirements.txt           # Python dependencies
```

---

## ðŸ‹ï¸â€â™‚ï¸ Training the Agent

To train the agent **from scratch**:

```bash
python train_snake_ai.py
```

To **resume training** from a previously saved model:

```bash
python train_snake_ai.py --model_path models/snake_model_30x30_trained.pth
```

This loads the weights from the given `.pth` file and continues training.

### â›” Keyboard Interrupt

If you stop training with `Ctrl+C`, the model will be saved automatically.

---

## ðŸŽ® Let the Model Play

Once you have a trained model, run:

```bash
python play_snake_ai.py --model_path models/snake_model_30x30_trained.pth
```

This opens a Pygame window where the model plays the game.

---

## ðŸ•¹ï¸ Play as Human

Want to play Snake yourself?

```bash
python play_snake_human.py
```

Use arrow keys to control the snake.

---

## ðŸ“Š Training Visualization

Training stats are plotted in real time using `matplotlib`:
- ðŸ“ˆ Reward per episode  
- ðŸ§® Score per episode  
- ðŸ“‰ Moving averages to visualize learning trends  

Plots refresh live during training, and optional static plots can be saved to the plots/ directory if enabled.

---

## ðŸ› ï¸ Requirements

Install required packages:

```bash
pip install -r requirements.txt
```

Minimum versions:

* Python 3.9+
* PyTorch >= 1.12
* Pygame
* Matplotlib
* NumPy

---

## ðŸ’¾ Model Saving Format

Trained models are saved as:

```
snake_model_30x30.pth
snake_model_30x30_1.pth
snake_model_30x30_2.pth
...
```

This avoids overwriting previous models.

---

## ðŸ“Œ TODO / Improvements

* [ ] Add target network for stable DQN
* [ ] Add experience replay prioritization
* [ ] Implement double DQN
* [ ] Add curriculum learning (e.g., smaller grid â†’ bigger grid)

---

## ðŸ“¢ License

MIT License Â© 2025 Usama

---

Let me know if you want a `requirements.txt` or badges (like Python version, license, etc.) added.
